# ğŸ§  Ø¯Ù„ÙŠÙ„ ØªÙƒØ§Ù…Ù„ Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø² (Reinforcement Learning)
# SAQR Seeker RL Integration Guide

**Ø§Ù„Ø¥ØµØ¯Ø§Ø±**: 1.0  
**Ø§Ù„ØªØ§Ø±ÙŠØ®**: 2026-01-12  
**Ø§Ù„Ù…Ø´Ø±ÙˆØ¹**: SAQR Seeker - Ù†Ø¸Ø§Ù… Ø§Ù„ØªØ­ÙƒÙ… Ø¨Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ø¹Ø²Ø²

---

# ÙÙ‡Ø±Ø³ Ø§Ù„Ù…Ø­ØªÙˆÙŠØ§Øª

1. [Ù…Ù‚Ø¯Ù…Ø© ÙˆÙÙƒØ±Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹](#Ø§Ù„ÙØµÙ„-1-Ù…Ù‚Ø¯Ù…Ø©-ÙˆÙÙƒØ±Ø©-Ø§Ù„Ù…Ø´Ø±ÙˆØ¹)
2. [Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª (Observation Space)](#Ø§Ù„ÙØµÙ„-2-Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª-observation-space)
3. [Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª (Action Space)](#Ø§Ù„ÙØµÙ„-3-Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª-action-space)
4. [Ø¯Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© (Reward Function)](#Ø§Ù„ÙØµÙ„-4-Ø¯Ø§Ù„Ø©-Ø§Ù„Ù…ÙƒØ§ÙØ£Ø©-reward-function)
5. [Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ¦Ø© (Environment)](#Ø§Ù„ÙØµÙ„-5-Ø¨Ù†Ø§Ø¡-Ø§Ù„Ø¨ÙŠØ¦Ø©-environment)
6. [Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª](#Ø§Ù„ÙØµÙ„-6-Ø§Ù„ØªØ¯Ø±ÙŠØ¨-ÙˆØ§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª)
7. [Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø§Ø±Ø¯ÙˆÙŠØ±](#Ø§Ù„ÙØµÙ„-7-Ø§Ù„Ù†Ø´Ø±-Ø¹Ù„Ù‰-Ø§Ù„Ù‡Ø§Ø±Ø¯ÙˆÙŠØ±)
8. [Ø§Ù„Ù…Ù„Ø§Ø­Ù‚](#Ø§Ù„Ù…Ù„Ø§Ø­Ù‚)

---

# Ø§Ù„ÙØµÙ„ 1: Ù…Ù‚Ø¯Ù…Ø© ÙˆÙÙƒØ±Ø© Ø§Ù„Ù…Ø´Ø±ÙˆØ¹

## 1.1 Ø§Ù„ÙÙƒØ±Ø© Ø§Ù„Ø¹Ø§Ù…Ø©

Ø§Ù„Ù‡Ø¯Ù Ù‡Ùˆ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ Ø§Ù„Ù…ØªØ­ÙƒÙ… Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ **PID** Ø¨Ù†Ù…ÙˆØ°Ø¬ **Reinforcement Learning** Ù„ÙŠÙƒÙˆÙ† "Ø§Ù„Ø¹Ù‚Ù„ Ø§Ù„Ù…ØªØ­ÙƒÙ…" Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ ÙÙŠ Ù†Ø¸Ø§Ù… SAQR Seeker.

### Ø§Ù„Ù…Ù‚Ø§Ø±Ù†Ø©

| Ø§Ù„Ø¬Ø§Ù†Ø¨ | PID Ø§Ù„ØªÙ‚Ù„ÙŠØ¯ÙŠ | RL Agent |
|--------|--------------|----------|
| **Ø§Ù„ØªÙƒÙŠÙ** | Ø«Ø§Ø¨ØªØŒ ÙŠØ­ØªØ§Ø¬ Ø¶Ø¨Ø· ÙŠØ¯ÙˆÙŠ | ÙŠØªØ¹Ù„Ù… ÙˆÙŠØªÙƒÙŠÙ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ |
| **Ø§Ù„ØªØ¹Ù‚ÙŠØ¯** | Ù…Ø­Ø¯ÙˆØ¯ Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®Ø·ÙŠ | ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹ Ø£Ù†Ø¸Ù…Ø© ØºÙŠØ± Ø®Ø·ÙŠØ© |
| **Ø§Ù„Ø§Ø³ØªØ¨Ø§Ù‚ÙŠØ©** | ØªÙØ§Ø¹Ù„ÙŠ ÙÙ‚Ø· | ÙŠØªÙ†Ø¨Ø£ ÙˆÙŠØ³ØªØ¨Ù‚ |
| **Ø§Ù„Ø¸Ø±ÙˆÙ Ø§Ù„Ù…ØªØºÙŠØ±Ø©** | ÙŠØ­ØªØ§Ø¬ Ø¥Ø¹Ø§Ø¯Ø© Ø¶Ø¨Ø· | ÙŠØªÙƒÙŠÙ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ |

## 1.2 Ù…Ø¹Ù…Ø§Ø±ÙŠØ© Ø§Ù„Ù†Ø¸Ø§Ù…

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ø­Ø§Ù„ÙŠ (PID Controller)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Sensors â†’ Detection â†’ Tracking â†’ [PID] â†’ Servo Commands

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Ø§Ù„ÙˆØ¶Ø¹ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ (RL Controller)                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
   Sensors â†’ Detection â†’ Tracking â†’ [RL Agent] â†’ Servo Commands
```

## 1.3 Ù…Ø±Ø§Ø­Ù„ Ø§Ù„ØªÙ†ÙÙŠØ°

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Ø§Ù„Ù…Ø±Ø­Ù„Ø© 1 â”‚    â”‚   Ø§Ù„Ù…Ø±Ø­Ù„Ø© 2 â”‚    â”‚   Ø§Ù„Ù…Ø±Ø­Ù„Ø© 3 â”‚
â”‚   Simulationâ”‚â”€â”€â”€â–¶â”‚   Sim2Real  â”‚â”€â”€â”€â–¶â”‚   Hardware  â”‚
â”‚   Training  â”‚    â”‚   Transfer  â”‚    â”‚   Deploy    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# Ø§Ù„ÙØµÙ„ 2: Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª (Observation Space)

## 2.1 Ù†Ø¸Ø±Ø© Ø¹Ø§Ù…Ø©

Ø§Ù„Ù€ RL Agent ÙŠØ­ØªØ§Ø¬ "Ø±Ø¤ÙŠØ©" ÙƒØ§Ù…Ù„Ø© Ù„Ø­Ø§Ù„Ø© Ø§Ù„Ù†Ø¸Ø§Ù… Ù„ÙŠØªØ®Ø° Ù‚Ø±Ø§Ø±Ø§Øª ØµØ­ÙŠØ­Ø©.

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Observation Vector (27-30 Ø¹Ù†ØµØ±)              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚   Target State  â”‚   Servo State   â”‚   Environment State         â”‚
â”‚   (9 Ø¹Ù†Ø§ØµØ±)     â”‚   (5 Ø¹Ù†Ø§ØµØ±)     â”‚   (13 Ø¹Ù†ØµØ±)                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## 2.2 Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ù‡Ø¯Ù (Target State)

### Ø§Ù„Ø­Ù‚ÙˆÙ„
| Ø§Ù„Ø­Ù‚Ù„ | Ø§Ù„Ù†ÙˆØ¹ | Ø§Ù„Ù†Ø·Ø§Ù‚ | Ø§Ù„ÙˆØµÙ | Ø§Ù„Ù…ØµØ¯Ø± ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ |
|-------|-------|--------|-------|-------------------|
| `target_x` | float | -1 to 1 | Ù…ÙˆÙ‚Ø¹ X Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ù…Ø±ÙƒØ² | `TrackingResult.boundingBox.centerX()` |
| `target_y` | float | -1 to 1 | Ù…ÙˆÙ‚Ø¹ Y Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù„Ù…Ø±ÙƒØ² | `TrackingResult.boundingBox.centerY()` |
| `target_width` | float | 0 to 1 | Ø¹Ø±Ø¶ Ø§Ù„Ù‡Ø¯Ù (normalized) | `TrackingResult.boundingBox.width()` |
| `target_height` | float | 0 to 1 | Ø§Ø±ØªÙØ§Ø¹ Ø§Ù„Ù‡Ø¯Ù (normalized) | `TrackingResult.boundingBox.height()` |
| `target_vx` | float | -10 to 10 | Ø³Ø±Ø¹Ø© Ø§Ù„Ù‡Ø¯Ù Ø£ÙÙ‚ÙŠØ§Ù‹ | `ObjectTracker.velocity.x` |
| `target_vy` | float | -10 to 10 | Ø³Ø±Ø¹Ø© Ø§Ù„Ù‡Ø¯Ù Ø¹Ù…ÙˆØ¯ÙŠØ§Ù‹ | `ObjectTracker.velocity.y` |
| `confidence` | float | 0 to 1 | Ø«Ù‚Ø© Ø§Ù„ÙƒØ´Ù | `TrackingResult.confidence` |
| `target_visible` | bool | 0 or 1 | Ù‡Ù„ Ø§Ù„Ù‡Ø¯Ù Ù…Ø±Ø¦ÙŠ | `StableTracker.isTracking` |
| `frames_since_lost` | int | 0 to 100 | Ø¥Ø·Ø§Ø±Ø§Øª Ù…Ù†Ø° Ø§Ù„ÙÙ‚Ø¯Ø§Ù† | `StableTracker.lostFrames` |

### ÙƒÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
```kotlin
data class TargetObservation(
    val targetX: Float,
    val targetY: Float,
    val targetWidth: Float,
    val targetHeight: Float,
    val targetVx: Float,
    val targetVy: Float,
    val confidence: Float,
    val targetVisible: Float,
    val framesSinceLost: Float
)

fun extractTargetState(
    result: TrackingResult?,
    tracker: ObjectTracker,
    frameWidth: Int,
    frameHeight: Int
): TargetObservation {
    if (result == null) {
        return TargetObservation(
            targetX = 0f,
            targetY = 0f,
            targetWidth = 0f,
            targetHeight = 0f,
            targetVx = 0f,
            targetVy = 0f,
            confidence = 0f,
            targetVisible = 0f,
            framesSinceLost = tracker.lostFrames.toFloat()
        )
    }
    
    val box = result.boundingBox
    val centerX = (box.centerX() / frameWidth) * 2 - 1  // Normalize to [-1, 1]
    val centerY = (box.centerY() / frameHeight) * 2 - 1
    
    return TargetObservation(
        targetX = centerX,
        targetY = centerY,
        targetWidth = box.width() / frameWidth,
        targetHeight = box.height() / frameHeight,
        targetVx = tracker.velocity.x / 10f,  // Normalize
        targetVy = tracker.velocity.y / 10f,
        confidence = result.confidence,
        targetVisible = 1f,
        framesSinceLost = 0f
    )
}
```

---

## 2.3 Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø³ÙŠØ±ÙÙˆ (Servo State)

### Ø§Ù„Ø­Ù‚ÙˆÙ„
| Ø§Ù„Ø­Ù‚Ù„ | Ø§Ù„Ù†ÙˆØ¹ | Ø§Ù„Ù†Ø·Ø§Ù‚ | Ø§Ù„ÙˆØµÙ | Ø§Ù„Ù…ØµØ¯Ø± ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ |
|-------|-------|--------|-------|-------------------|
| `yaw_angle` | float | -100 to 100 | Ø²Ø§ÙˆÙŠØ© Yaw Ø§Ù„Ø­Ø§Ù„ÙŠØ© | `SharedBusManager.servoFeedback[0]` |
| `pitch_angle` | float | -100 to 100 | Ø²Ø§ÙˆÙŠØ© Pitch Ø§Ù„Ø­Ø§Ù„ÙŠØ© | `SharedBusManager.servoFeedback[1]` |
| `roll_angle` | float | -100 to 100 | Ø²Ø§ÙˆÙŠØ© Roll Ø§Ù„Ø­Ø§Ù„ÙŠØ© | `SharedBusManager.servoFeedback[2]` |
| `yaw_velocity` | float | -10 to 10 | Ø³Ø±Ø¹Ø© Yaw | Ù…Ø­Ø³ÙˆØ¨ Ù…Ù† Ø§Ù„ØªØºÙŠØ± |
| `pitch_velocity` | float | -10 to 10 | Ø³Ø±Ø¹Ø© Pitch | Ù…Ø­Ø³ÙˆØ¨ Ù…Ù† Ø§Ù„ØªØºÙŠØ± |

### ÙƒÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
```kotlin
data class ServoObservation(
    val yawAngle: Float,
    val pitchAngle: Float,
    val rollAngle: Float,
    val yawVelocity: Float,
    val pitchVelocity: Float
)

class ServoStateExtractor {
    private var lastYaw = 0f
    private var lastPitch = 0f
    private var lastTimestamp = 0L
    
    fun extract(busManager: SharedBusManager): ServoObservation {
        val currentTime = System.currentTimeMillis()
        val dt = (currentTime - lastTimestamp) / 1000f
        
        val yaw = busManager.getServoFeedback(1)
        val pitch = busManager.getServoFeedback(2)
        val roll = busManager.getServoFeedback(3)
        
        val yawVel = if (dt > 0) (yaw - lastYaw) / dt else 0f
        val pitchVel = if (dt > 0) (pitch - lastPitch) / dt else 0f
        
        lastYaw = yaw
        lastPitch = pitch
        lastTimestamp = currentTime
        
        return ServoObservation(
            yawAngle = yaw / 100f,      // Normalize to [-1, 1]
            pitchAngle = pitch / 100f,
            rollAngle = roll / 100f,
            yawVelocity = yawVel / 10f,
            pitchVelocity = pitchVel / 10f
        )
    }
}
```

---

## 2.4 Ø¨ÙŠØ§Ù†Ø§Øª IMU (Environment State)

### Ø§Ù„Ø­Ù‚ÙˆÙ„
| Ø§Ù„Ø­Ù‚Ù„ | Ø§Ù„Ù†ÙˆØ¹ | Ø§Ù„Ù†Ø·Ø§Ù‚ | Ø§Ù„ÙˆØµÙ | Ø§Ù„Ù…ØµØ¯Ø± ÙÙŠ Ø§Ù„Ù…Ø´Ø±ÙˆØ¹ |
|-------|-------|--------|-------|-------------------|
| `gyro_x` | float | -5 to 5 | Ø³Ø±Ø¹Ø© Ø§Ù„Ø¯ÙˆØ±Ø§Ù† X | `GyroManager.gyroX` |
| `gyro_y` | float | -5 to 5 | Ø³Ø±Ø¹Ø© Ø§Ù„Ø¯ÙˆØ±Ø§Ù† Y | `GyroManager.gyroY` |
| `gyro_z` | float | -5 to 5 | Ø³Ø±Ø¹Ø© Ø§Ù„Ø¯ÙˆØ±Ø§Ù† Z | `GyroManager.gyroZ` |
| `accel_x` | float | -20 to 20 | Ø§Ù„ØªØ³Ø§Ø±Ø¹ X | `AccelerometerManager.x` |
| `accel_y` | float | -20 to 20 | Ø§Ù„ØªØ³Ø§Ø±Ø¹ Y | `AccelerometerManager.y` |
| `accel_z` | float | -20 to 20 | Ø§Ù„ØªØ³Ø§Ø±Ø¹ Z | `AccelerometerManager.z` |
| `shake_level` | float | 0 to 1 | Ù…Ø³ØªÙˆÙ‰ Ø§Ù„Ø§Ù‡ØªØ²Ø§Ø² | `SensorFusionLayer.shakeLevel` |

### ÙƒÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
```kotlin
data class IMUObservation(
    val gyroX: Float,
    val gyroY: Float,
    val gyroZ: Float,
    val accelX: Float,
    val accelY: Float,
    val accelZ: Float,
    val shakeLevel: Float
)

fun extractIMUState(
    gyroManager: GyroManager,
    accelManager: AccelerometerManager
): IMUObservation {
    val gyro = gyroManager.getLatestValues()
    val accel = accelManager.getLatestValues()
    
    // Calculate shake level from acceleration variance
    val shakeLevel = sqrt(
        (accel.x - 0).pow(2) + 
        (accel.y - 0).pow(2) + 
        (accel.z - 9.8f).pow(2)
    ) / 10f
    
    return IMUObservation(
        gyroX = gyro.x / 5f,      // Normalize
        gyroY = gyro.y / 5f,
        gyroZ = gyro.z / 5f,
        accelX = accel.x / 20f,
        accelY = accel.y / 20f,
        accelZ = accel.z / 20f,
        shakeLevel = shakeLevel.coerceIn(0f, 1f)
    )
}
```

---

## 2.5 Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø·Ø£ (Error State)

### Ø§Ù„Ø­Ù‚ÙˆÙ„
| Ø§Ù„Ø­Ù‚Ù„ | Ø§Ù„Ù†ÙˆØ¹ | Ø§Ù„Ù†Ø·Ø§Ù‚ | Ø§Ù„ÙˆØµÙ |
|-------|-------|--------|-------|
| `error_x` | float | -1 to 1 | Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø£ÙÙ‚ÙŠ |
| `error_y` | float | -1 to 1 | Ø§Ù„Ø®Ø·Ø£ Ø§Ù„Ø¹Ù…ÙˆØ¯ÙŠ |
| `error_integral_x` | float | -10 to 10 | ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø®Ø·Ø£ X |
| `error_integral_y` | float | -10 to 10 | ØªØ±Ø§ÙƒÙ… Ø§Ù„Ø®Ø·Ø£ Y |
| `error_derivative_x` | float | -10 to 10 | ØªØºÙŠØ± Ø§Ù„Ø®Ø·Ø£ X |
| `error_derivative_y` | float | -10 to 10 | ØªØºÙŠØ± Ø§Ù„Ø®Ø·Ø£ Y |
| `time_on_target` | int | 0 to 1000 | Ø¥Ø·Ø§Ø±Ø§Øª Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù |

### ÙƒÙˆØ¯ Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬
```kotlin
class ErrorStateExtractor {
    private var integralX = 0f
    private var integralY = 0f
    private var lastErrorX = 0f
    private var lastErrorY = 0f
    private var timeOnTarget = 0
    
    fun extract(targetX: Float, targetY: Float): ErrorObservation {
        val errorX = -targetX  // Error = desired (0) - actual
        val errorY = -targetY
        
        integralX += errorX * 0.033f  // dt â‰ˆ 33ms
        integralY += errorY * 0.033f
        
        val derivativeX = (errorX - lastErrorX) / 0.033f
        val derivativeY = (errorY - lastErrorY) / 0.033f
        
        // Clamp integral to prevent windup
        integralX = integralX.coerceIn(-10f, 10f)
        integralY = integralY.coerceIn(-10f, 10f)
        
        // Track time on target
        if (abs(errorX) < 0.1f && abs(errorY) < 0.1f) {
            timeOnTarget++
        } else {
            timeOnTarget = 0
        }
        
        lastErrorX = errorX
        lastErrorY = errorY
        
        return ErrorObservation(
            errorX = errorX,
            errorY = errorY,
            errorIntegralX = integralX / 10f,
            errorIntegralY = integralY / 10f,
            errorDerivativeX = derivativeX / 10f,
            errorDerivativeY = derivativeY / 10f,
            timeOnTarget = (timeOnTarget / 1000f).coerceIn(0f, 1f)
        )
    }
}
```

---

## 2.6 ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù€ Observation Ø§Ù„ÙƒØ§Ù…Ù„

```kotlin
class ObservationBuilder(
    private val servoExtractor: ServoStateExtractor,
    private val errorExtractor: ErrorStateExtractor
) {
    fun build(
        trackingResult: TrackingResult?,
        tracker: ObjectTracker,
        busManager: SharedBusManager,
        gyroManager: GyroManager,
        accelManager: AccelerometerManager,
        frameWidth: Int,
        frameHeight: Int
    ): FloatArray {
        val target = extractTargetState(trackingResult, tracker, frameWidth, frameHeight)
        val servo = servoExtractor.extract(busManager)
        val imu = extractIMUState(gyroManager, accelManager)
        val error = errorExtractor.extract(target.targetX, target.targetY)
        
        return floatArrayOf(
            // Target (9)
            target.targetX, target.targetY,
            target.targetWidth, target.targetHeight,
            target.targetVx, target.targetVy,
            target.confidence, target.targetVisible, target.framesSinceLost,
            
            // Servo (5)
            servo.yawAngle, servo.pitchAngle, servo.rollAngle,
            servo.yawVelocity, servo.pitchVelocity,
            
            // IMU (7)
            imu.gyroX, imu.gyroY, imu.gyroZ,
            imu.accelX, imu.accelY, imu.accelZ,
            imu.shakeLevel,
            
            // Error (7)
            error.errorX, error.errorY,
            error.errorIntegralX, error.errorIntegralY,
            error.errorDerivativeX, error.errorDerivativeY,
            error.timeOnTarget
        )
    }
}
```

---

# Ø§Ù„ÙØµÙ„ 3: Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª (Action Space)

## 3.1 Ø£Ù†ÙˆØ§Ø¹ Ù…Ø³Ø§Ø­Ø© Ø§Ù„Ø¥Ø¬Ø±Ø§Ø¡Ø§Øª

### Ø§Ù„Ø®ÙŠØ§Ø± 1: ØªØ­ÙƒÙ… Ø¨Ø§Ù„Ø²Ø§ÙˆÙŠØ© Ø§Ù„Ù…Ø·Ù„Ù‚Ø© (Position Control)
```python
# Action = Ø§Ù„Ø²Ø§ÙˆÙŠØ© Ø§Ù„Ù…Ø³ØªÙ‡Ø¯ÙØ© Ù…Ø¨Ø§Ø´Ø±Ø©
action_space = spaces.Box(
    low=np.array([-100, -100, -100]),   # Yaw, Pitch, Roll (degrees)
    high=np.array([100, 100, 100]),
    dtype=np.float32
)
```

### Ø§Ù„Ø®ÙŠØ§Ø± 2: ØªØ­ÙƒÙ… Ø¨Ø§Ù„ØªØºÙŠÙŠØ± (Delta Control) âœ… Ù…ÙˆØµÙ‰ Ø¨Ù‡
```python
# Action = Ø§Ù„ØªØºÙŠÙŠØ± ÙÙŠ Ø§Ù„Ø²Ø§ÙˆÙŠØ©
action_space = spaces.Box(
    low=np.array([-1, -1, -1]),   # Normalized
    high=np.array([1, 1, 1]),
    dtype=np.float32
)
# Ø«Ù… ÙŠÙØ¶Ø±Ø¨ ÙÙŠ scale (Ù…Ø«Ù„Ø§Ù‹ 5Â°)
```

### Ø§Ù„Ø®ÙŠØ§Ø± 3: ØªØ­ÙƒÙ… Ø¨Ø§Ù„Ø³Ø±Ø¹Ø© (Velocity Control)
```python
# Action = Ø³Ø±Ø¹Ø© Ø§Ù„Ø¯ÙˆØ±Ø§Ù†
action_space = spaces.Box(
    low=np.array([-1, -1, -1]),   # Normalized velocity
    high=np.array([1, 1, 1]),
    dtype=np.float32
)
# Ø«Ù… ÙŠÙØ¶Ø±Ø¨ ÙÙŠ max_velocity (Ù…Ø«Ù„Ø§Ù‹ 100Â°/s)
```

---

## 3.2 ØªØ­ÙˆÙŠÙ„ Action Ø¥Ù„Ù‰ Ø£ÙˆØ§Ù…Ø± Ø³ÙŠØ±ÙÙˆ

```kotlin
class ActionExecutor(
    private val busManager: SharedBusManager
) {
    private val ACTION_SCALE = 5f  // Â±5Â° per action unit
    private val MAX_ANGLE = 100f
    private val MIN_ANGLE = -100f
    
    private var currentYaw = 0f
    private var currentPitch = 0f
    private var currentRoll = 0f
    
    fun execute(action: FloatArray) {
        // Scale actions
        val deltaYaw = action[0] * ACTION_SCALE
        val deltaPitch = action[1] * ACTION_SCALE
        val deltaRoll = action[2] * ACTION_SCALE
        
        // Apply deltas with clamping
        currentYaw = (currentYaw + deltaYaw).coerceIn(MIN_ANGLE, MAX_ANGLE)
        currentPitch = (currentPitch + deltaPitch).coerceIn(MIN_ANGLE, MAX_ANGLE)
        currentRoll = (currentRoll + deltaRoll).coerceIn(MIN_ANGLE, MAX_ANGLE)
        
        // Send to servos
        busManager.moveServo(1, currentYaw)
        busManager.moveServo(2, currentPitch)
        busManager.moveServo(3, currentRoll)
    }
    
    fun reset() {
        currentYaw = 0f
        currentPitch = 0f
        currentRoll = 0f
        busManager.moveServo(1, 0f)
        busManager.moveServo(2, 0f)
        busManager.moveServo(3, 0f)
    }
}
```

---

## 3.3 Ù„Ù„ØªØ­ÙƒÙ… Ø¨Ø§Ù„Ø·Ø§Ø¦Ø±Ø© (Drone)

```python
# Action Space Ù„Ù„Ø·ÙŠØ±Ø§Ù†
action_space = spaces.Box(
    low=np.array([0, -1, -1, -1]),    # [Throttle, Roll, Pitch, Yaw]
    high=np.array([1, 1, 1, 1]),
    dtype=np.float32
)
```

```kotlin
class DroneActionExecutor(
    private val flightController: STM32FlightController
) {
    fun execute(action: FloatArray) {
        val throttle = (action[0] * 1000).toInt()  // 0-1000
        val roll = (action[1] * 500).toInt()       // -500 to 500
        val pitch = (action[2] * 500).toInt()      // -500 to 500
        val yaw = (action[3] * 500).toInt()        // -500 to 500
        
        flightController.sendCommand(throttle, roll, pitch, yaw)
    }
}
```

---

# Ø§Ù„ÙØµÙ„ 4: Ø¯Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© (Reward Function)

## 4.1 Ø§Ù„Ù…Ø¨Ø§Ø¯Ø¦ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠØ©

| Ø§Ù„Ù…Ø¨Ø¯Ø£ | Ø§Ù„ÙˆØµÙ |
|--------|-------|
| **Ø§Ù„Ù‡Ø¯Ù ÙÙŠ Ø§Ù„Ù…Ø±ÙƒØ²** | Ù…ÙƒØ§ÙØ£Ø© ÙƒØ¨ÙŠØ±Ø© Ø¹Ù†Ø¯ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø®Ø·Ø£ |
| **Ø§Ù„Ø§Ø³ØªÙ‚Ø±Ø§Ø±** | Ø¹Ù‚ÙˆØ¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙƒØ§Øª Ø§Ù„Ù…ÙØ±Ø·Ø© |
| **Ø§Ù„Ù…ØªØ§Ø¨Ø¹Ø© Ø§Ù„Ù…Ø³ØªÙ…Ø±Ø©** | Ù…ÙƒØ§ÙØ£Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø­ÙØ§Ø¸ Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù |
| **ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ù‡Ø¯Ù** | Ø¹Ù‚ÙˆØ¨Ø© ÙƒØ¨ÙŠØ±Ø© |

## 4.2 Ø¯Ø§Ù„Ø© Ø§Ù„Ù…ÙƒØ§ÙØ£Ø© Ø§Ù„ÙƒØ§Ù…Ù„Ø©

```python
def calculate_reward(
    state: np.ndarray,
    action: np.ndarray,
    next_state: np.ndarray,
    info: dict
) -> float:
    reward = 0.0
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 1. Ù…ÙƒØ§ÙØ£Ø© Ø¹Ù„Ù‰ ØªÙ‚Ù„ÙŠÙ„ Ø§Ù„Ø®Ø·Ø£ (MAIN OBJECTIVE)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    error_x = next_state[20]  # error_x index
    error_y = next_state[21]  # error_y index
    error_distance = np.sqrt(error_x**2 + error_y**2)
    
    # Exponential reward: max 1.0 when on target
    tracking_reward = np.exp(-5 * error_distance)
    reward += tracking_reward * 2.0  # Weight: 2.0
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 2. Ù…ÙƒØ§ÙØ£Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© (PRECISION BONUS)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    if error_distance < 0.05:  # Within 5% of center
        reward += 3.0  # Big bonus for precision
    elif error_distance < 0.1:  # Within 10%
        reward += 1.5
    elif error_distance < 0.2:  # Within 20%
        reward += 0.5
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 3. Ù…ÙƒØ§ÙØ£Ø© Ø¹Ù„Ù‰ Ø±Ø¤ÙŠØ© Ø§Ù„Ù‡Ø¯Ù (VISIBILITY)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    target_visible = next_state[7]  # target_visible index
    if target_visible > 0.5:
        reward += 0.5
    else:
        reward -= 2.0  # Penalty for losing target
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 4. Ø¹Ù‚ÙˆØ¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø­Ø±ÙƒØ© Ø§Ù„Ø²Ø§Ø¦Ø¯Ø© (SMOOTHNESS)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    action_magnitude = np.sum(np.abs(action))
    reward -= 0.1 * action_magnitude
    
    # Extra penalty for jerky movements
    if np.max(np.abs(action)) > 0.8:
        reward -= 0.3
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 5. Ù…ÙƒØ§ÙØ£Ø© Ø¹Ù„Ù‰ ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø®Ø·Ø£ (IMPROVEMENT)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    prev_error_x = state[20]
    prev_error_y = state[21]
    prev_error_dist = np.sqrt(prev_error_x**2 + prev_error_y**2)
    
    improvement = prev_error_dist - error_distance
    reward += improvement * 5.0  # Reward improvement
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 6. Ø¹Ù‚ÙˆØ¨Ø© Ø¹Ù„Ù‰ ÙÙ‚Ø¯Ø§Ù† Ø§Ù„Ù‡Ø¯Ù Ø·ÙˆÙŠÙ„Ø§Ù‹ (TIMEOUT)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    frames_since_lost = next_state[8]
    if frames_since_lost > 10:
        reward -= 0.5 * (frames_since_lost / 10)
    if frames_since_lost > 30:
        reward -= 10.0  # Heavy penalty
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 7. Ù…ÙƒØ§ÙØ£Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Øª Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø¯Ù (TIME ON TARGET)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    time_on_target = next_state[26]  # time_on_target index
    reward += time_on_target * 2.0
    
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    # 8. Ø¹Ù‚ÙˆØ¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø§Ù‡ØªØ²Ø§Ø² Ø§Ù„Ø¹Ø§Ù„ÙŠ (STABILITY)
    # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
    shake_level = next_state[19]  # shake_level index
    if shake_level > 0.5:
        reward -= shake_level * 0.5
    
    return reward
```

---

## 4.3 Reward Shaping Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø³Ø±ÙŠØ¹

```python
class RewardShaper:
    def __init__(self):
        self.curriculum_stage = 0
        self.episodes_at_stage = 0
        
    def get_reward(self, state, action, next_state, info):
        base_reward = calculate_reward(state, action, next_state, info)
        
        # Curriculum Learning Stages
        if self.curriculum_stage == 0:
            # Stage 0: Just track stationary target
            return base_reward * 2.0  # Amplify for easier learning
            
        elif self.curriculum_stage == 1:
            # Stage 1: Track slow-moving target
            return base_reward * 1.5
            
        elif self.curriculum_stage == 2:
            # Stage 2: Track fast-moving target
            return base_reward * 1.2
            
        else:
            # Stage 3: Full complexity
            return base_reward
    
    def maybe_advance_stage(self, success_rate):
        self.episodes_at_stage += 1
        if success_rate > 0.8 and self.episodes_at_stage > 100:
            self.curriculum_stage = min(self.curriculum_stage + 1, 3)
            self.episodes_at_stage = 0
            print(f"Advanced to curriculum stage {self.curriculum_stage}")
```

---

# Ø§Ù„ÙØµÙ„ 5: Ø¨Ù†Ø§Ø¡ Ø§Ù„Ø¨ÙŠØ¦Ø© (Environment)

## 5.1 Ø¨ÙŠØ¦Ø© Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© (Simulation)

```python
import gymnasium as gym
from gymnasium import spaces
import numpy as np

class SAQRGimbalEnv(gym.Env):
    """
    Ø¨ÙŠØ¦Ø© Ù…Ø­Ø§ÙƒØ§Ø© Ù„Ù†Ø¸Ø§Ù… SAQR Gimbal Ù„Ù„ØªØ¯Ø±ÙŠØ¨.
    """
    
    metadata = {"render_modes": ["human", "rgb_array"], "render_fps": 30}
    
    def __init__(self, render_mode=None):
        super().__init__()
        
        self.render_mode = render_mode
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Observation Space (27 elements)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.observation_space = spaces.Box(
            low=np.array([
                # Target (9)
                -1, -1,           # target_x, target_y
                0, 0,             # target_w, target_h
                -1, -1,           # target_vx, target_vy
                0,                # confidence
                0,                # target_visible
                0,                # frames_since_lost
                # Servo (5)
                -1, -1, -1,       # yaw, pitch, roll
                -1, -1,           # yaw_vel, pitch_vel
                # IMU (7)
                -1, -1, -1,       # gyro
                -1, -1, -1,       # accel
                0,                # shake
                # Error (7)
                -1, -1,           # error
                -1, -1,           # integral
                -1, -1,           # derivative
                0,                # time_on_target
            ], dtype=np.float32),
            high=np.array([
                # Target (9)
                1, 1,             # target_x, target_y
                1, 1,             # target_w, target_h
                1, 1,             # target_vx, target_vy
                1,                # confidence
                1,                # target_visible
                1,                # frames_since_lost
                # Servo (5)
                1, 1, 1,          # yaw, pitch, roll
                1, 1,             # yaw_vel, pitch_vel
                # IMU (7)
                1, 1, 1,          # gyro
                1, 1, 1,          # accel
                1,                # shake
                # Error (7)
                1, 1,             # error
                1, 1,             # integral
                1, 1,             # derivative
                1,                # time_on_target
            ], dtype=np.float32),
        )
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # Action Space (3 elements: yaw_delta, pitch_delta, roll_delta)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.action_space = spaces.Box(
            low=-1.0, high=1.0, shape=(3,), dtype=np.float32
        )
        
        # Internal state
        self.reset()
    
    def reset(self, seed=None, options=None):
        super().reset(seed=seed)
        
        # Initialize target at random position
        self.target_x = self.np_random.uniform(-0.5, 0.5)
        self.target_y = self.np_random.uniform(-0.5, 0.5)
        self.target_vx = self.np_random.uniform(-0.02, 0.02)
        self.target_vy = self.np_random.uniform(-0.02, 0.02)
        
        # Initialize gimbal at center
        self.gimbal_yaw = 0.0
        self.gimbal_pitch = 0.0
        self.gimbal_roll = 0.0
        
        # Initialize counters
        self.steps = 0
        self.time_on_target = 0
        self.frames_since_lost = 0
        self.error_integral = np.array([0.0, 0.0])
        self.last_error = np.array([0.0, 0.0])
        
        observation = self._get_observation()
        info = {}
        
        return observation, info
    
    def step(self, action):
        self.steps += 1
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 1. Apply action to gimbal
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        action_scale = 0.05  # 5% of range per action
        self.gimbal_yaw += action[0] * action_scale
        self.gimbal_pitch += action[1] * action_scale
        self.gimbal_roll += action[2] * action_scale
        
        # Clamp gimbal angles
        self.gimbal_yaw = np.clip(self.gimbal_yaw, -1, 1)
        self.gimbal_pitch = np.clip(self.gimbal_pitch, -1, 1)
        self.gimbal_roll = np.clip(self.gimbal_roll, -1, 1)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 2. Update target position
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        self.target_x += self.target_vx
        self.target_y += self.target_vy
        
        # Bounce off edges
        if abs(self.target_x) > 0.9:
            self.target_vx *= -1
        if abs(self.target_y) > 0.9:
            self.target_vy *= -1
        
        # Random velocity changes
        if self.np_random.random() < 0.05:
            self.target_vx += self.np_random.uniform(-0.01, 0.01)
            self.target_vy += self.np_random.uniform(-0.01, 0.01)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 3. Calculate apparent target position (relative to gimbal)
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        apparent_x = self.target_x - self.gimbal_yaw
        apparent_y = self.target_y - self.gimbal_pitch
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 4. Check if target is visible
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        target_visible = abs(apparent_x) < 0.5 and abs(apparent_y) < 0.5
        
        if target_visible:
            self.frames_since_lost = 0
            error = np.sqrt(apparent_x**2 + apparent_y**2)
            if error < 0.1:
                self.time_on_target += 1
            else:
                self.time_on_target = max(0, self.time_on_target - 1)
        else:
            self.frames_since_lost += 1
            self.time_on_target = 0
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 5. Calculate reward
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        observation = self._get_observation()
        reward = self._calculate_reward(action, apparent_x, apparent_y, target_visible)
        
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        # 6. Check termination
        # â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
        terminated = self.frames_since_lost > 30
        truncated = self.steps >= 1000
        
        info = {
            "error": np.sqrt(apparent_x**2 + apparent_y**2),
            "target_visible": target_visible,
            "time_on_target": self.time_on_target,
        }
        
        return observation, reward, terminated, truncated, info
    
    def _get_observation(self):
        apparent_x = self.target_x - self.gimbal_yaw
        apparent_y = self.target_y - self.gimbal_pitch
        target_visible = abs(apparent_x) < 0.5 and abs(apparent_y) < 0.5
        
        # Calculate error derivatives
        error = np.array([apparent_x, apparent_y])
        self.error_integral += error * 0.033
        self.error_integral = np.clip(self.error_integral, -1, 1)
        error_derivative = (error - self.last_error) / 0.033
        self.last_error = error.copy()
        
        return np.array([
            # Target (9)
            apparent_x if target_visible else 0,
            apparent_y if target_visible else 0,
            0.1, 0.1,  # target size
            self.target_vx, self.target_vy,
            0.9 if target_visible else 0,  # confidence
            1.0 if target_visible else 0,
            self.frames_since_lost / 30.0,
            # Servo (5)
            self.gimbal_yaw,
            self.gimbal_pitch,
            self.gimbal_roll,
            0, 0,  # velocities
            # IMU (7)
            0, 0, 0,  # gyro
            0, 0, 1.0,  # accel (gravity)
            0,  # shake
            # Error (7)
            apparent_x if target_visible else 0,
            apparent_y if target_visible else 0,
            self.error_integral[0],
            self.error_integral[1],
            np.clip(error_derivative[0], -1, 1),
            np.clip(error_derivative[1], -1, 1),
            self.time_on_target / 100.0,
        ], dtype=np.float32)
    
    def _calculate_reward(self, action, apparent_x, apparent_y, target_visible):
        reward = 0.0
        
        error_dist = np.sqrt(apparent_x**2 + apparent_y**2)
        
        # Tracking reward
        if target_visible:
            reward += np.exp(-5 * error_dist) * 2.0
            if error_dist < 0.05:
                reward += 3.0
            elif error_dist < 0.1:
                reward += 1.5
        else:
            reward -= 2.0
        
        # Smoothness penalty
        reward -= 0.1 * np.sum(np.abs(action))
        
        # Time on target bonus
        reward += self.time_on_target * 0.02
        
        return reward
```

---

## 5.2 ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ¦Ø©

```python
from gymnasium.envs.registration import register

register(
    id="SAQRGimbal-v0",
    entry_point="saqr_env:SAQRGimbalEnv",
    max_episode_steps=1000,
)
```

---

# Ø§Ù„ÙØµÙ„ 6: Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ§Øª

## 6.1 Ø§Ø®ØªÙŠØ§Ø± Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ©

| Ø§Ù„Ø®ÙˆØ§Ø±Ø²Ù…ÙŠØ© | Ø§Ù„Ù…Ù…ÙŠØ²Ø§Øª | Ø§Ù„Ø¹ÙŠÙˆØ¨ | Ø§Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… |
|------------|----------|--------|-----------|
| **PPO** | Ù…Ø³ØªÙ‚Ø±ØŒ Ø³Ù‡Ù„ Ø§Ù„Ø¶Ø¨Ø· | Ø£Ø¨Ø·Ø£ Ù…Ù† Off-Policy | âœ… Ù„Ù„Ø¨Ø¯Ø§ÙŠØ© |
| **SAC** | Sample Efficient | Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ø§Ù‹ | âœ… Ù„Ù„Ø¯Ù‚Ø© Ø§Ù„Ø¹Ø§Ù„ÙŠØ© |
| **TD3** | Ù…Ù†Ø§Ø³Ø¨ Ù„Ù„Ù€ Continuous | Ø­Ø³Ø§Ø³ Ù„Ù„Ù€ Hyperparameters | Ù„Ù„Ø³Ø±Ø¹Ø© |
| **DDPG** | Ø¨Ø³ÙŠØ· | ØºÙŠØ± Ù…Ø³ØªÙ‚Ø± | ØºÙŠØ± Ù…ÙˆØµÙ‰ |

## 6.2 Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… PPO

```python
from stable_baselines3 import PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback

# Create vectorized environment
env = make_vec_env("SAQRGimbal-v0", n_envs=8)

# Create evaluation environment
eval_env = make_vec_env("SAQRGimbal-v0", n_envs=1)

# Callbacks
eval_callback = EvalCallback(
    eval_env,
    best_model_save_path="./models/best/",
    log_path="./logs/",
    eval_freq=10000,
    deterministic=True,
    render=False,
)

checkpoint_callback = CheckpointCallback(
    save_freq=50000,
    save_path="./models/checkpoints/",
    name_prefix="ppo_gimbal",
)

# Create and train model
model = PPO(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    n_steps=2048,
    batch_size=64,
    n_epochs=10,
    gamma=0.99,
    gae_lambda=0.95,
    clip_range=0.2,
    ent_coef=0.01,
    vf_coef=0.5,
    max_grad_norm=0.5,
    verbose=1,
    tensorboard_log="./logs/tensorboard/",
)

# Train
model.learn(
    total_timesteps=5_000_000,
    callback=[eval_callback, checkpoint_callback],
    progress_bar=True,
)

# Save final model
model.save("models/ppo_gimbal_final")
```

---

## 6.3 Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… SAC

```python
from stable_baselines3 import SAC

model = SAC(
    "MlpPolicy",
    env,
    learning_rate=3e-4,
    buffer_size=1_000_000,
    learning_starts=10000,
    batch_size=256,
    tau=0.005,
    gamma=0.99,
    train_freq=1,
    gradient_steps=1,
    ent_coef="auto",
    verbose=1,
    tensorboard_log="./logs/tensorboard/",
)

model.learn(total_timesteps=2_000_000)
model.save("models/sac_gimbal_final")
```

---

## 6.4 Hyperparameter Tuning

```python
import optuna
from stable_baselines3 import PPO
from stable_baselines3.common.evaluation import evaluate_policy

def objective(trial):
    # Sample hyperparameters
    learning_rate = trial.suggest_float("learning_rate", 1e-5, 1e-3, log=True)
    n_steps = trial.suggest_categorical("n_steps", [512, 1024, 2048, 4096])
    batch_size = trial.suggest_categorical("batch_size", [32, 64, 128, 256])
    gamma = trial.suggest_float("gamma", 0.95, 0.999)
    gae_lambda = trial.suggest_float("gae_lambda", 0.9, 0.99)
    clip_range = trial.suggest_float("clip_range", 0.1, 0.3)
    ent_coef = trial.suggest_float("ent_coef", 0.001, 0.1, log=True)
    
    env = make_vec_env("SAQRGimbal-v0", n_envs=4)
    
    model = PPO(
        "MlpPolicy",
        env,
        learning_rate=learning_rate,
        n_steps=n_steps,
        batch_size=batch_size,
        gamma=gamma,
        gae_lambda=gae_lambda,
        clip_range=clip_range,
        ent_coef=ent_coef,
        verbose=0,
    )
    
    model.learn(total_timesteps=100_000)
    
    mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)
    
    return mean_reward

study = optuna.create_study(direction="maximize")
study.optimize(objective, n_trials=50)

print("Best hyperparameters:", study.best_params)
```

---

# Ø§Ù„ÙØµÙ„ 7: Ø§Ù„Ù†Ø´Ø± Ø¹Ù„Ù‰ Ø§Ù„Ù‡Ø§Ø±Ø¯ÙˆÙŠØ±

## 7.1 ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù€ TFLite

```python
import tensorflow as tf
from stable_baselines3 import PPO
import numpy as np

# Load trained model
model = PPO.load("models/ppo_gimbal_final")

# Extract policy network
policy = model.policy

# Create TF function
class PolicyWrapper(tf.Module):
    def __init__(self, policy):
        self.policy = policy
        
    @tf.function(input_signature=[tf.TensorSpec([1, 27], tf.float32)])
    def predict(self, observation):
        action, _ = self.policy.predict(observation, deterministic=True)
        return action

wrapper = PolicyWrapper(policy)

# Convert to TFLite
converter = tf.lite.TFLiteConverter.from_concrete_functions(
    [wrapper.predict.get_concrete_function()]
)
converter.target_spec.supported_ops = [
    tf.lite.OpsSet.TFLITE_BUILTINS,
    tf.lite.OpsSet.SELECT_TF_OPS,
]
converter.optimizations = [tf.lite.Optimize.DEFAULT]

tflite_model = converter.convert()

# Save
with open("models/rl_policy.tflite", "wb") as f:
    f.write(tflite_model)
```

---

## 7.2 ØªØ´ØºÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¹Ù„Ù‰ Android

```kotlin
class RLController(context: Context) {
    private val interpreter: Interpreter
    private val inputBuffer: ByteBuffer
    private val outputBuffer: ByteBuffer
    
    init {
        // Load TFLite model
        val modelFile = loadModelFile(context, "rl_policy.tflite")
        interpreter = Interpreter(modelFile)
        
        // Allocate buffers
        inputBuffer = ByteBuffer.allocateDirect(27 * 4)  // 27 floats
        inputBuffer.order(ByteOrder.nativeOrder())
        
        outputBuffer = ByteBuffer.allocateDirect(3 * 4)  // 3 floats
        outputBuffer.order(ByteOrder.nativeOrder())
    }
    
    fun predict(observation: FloatArray): FloatArray {
        // Prepare input
        inputBuffer.rewind()
        for (value in observation) {
            inputBuffer.putFloat(value)
        }
        
        // Run inference
        inputBuffer.rewind()
        outputBuffer.rewind()
        interpreter.run(inputBuffer, outputBuffer)
        
        // Extract output
        outputBuffer.rewind()
        val action = FloatArray(3)
        for (i in 0 until 3) {
            action[i] = outputBuffer.getFloat()
        }
        
        return action
    }
    
    private fun loadModelFile(context: Context, filename: String): MappedByteBuffer {
        val assetManager = context.assets
        val fileDescriptor = assetManager.openFd(filename)
        val inputStream = FileInputStream(fileDescriptor.fileDescriptor)
        val fileChannel = inputStream.channel
        return fileChannel.map(
            FileChannel.MapMode.READ_ONLY,
            fileDescriptor.startOffset,
            fileDescriptor.declaredLength
        )
    }
}
```

---

## 7.3 ØªÙƒØ§Ù…Ù„ Ù…Ø¹ Ø§Ù„Ù†Ø¸Ø§Ù… Ø§Ù„Ø­Ø§Ù„ÙŠ

```kotlin
class HybridController(
    context: Context,
    private val busManager: SharedBusManager,
    private val observationBuilder: ObservationBuilder
) {
    private val rlController = RLController(context)
    private val actionExecutor = ActionExecutor(busManager)
    
    private var useRL = true  // Toggle between RL and PID
    
    fun update(
        trackingResult: TrackingResult?,
        tracker: ObjectTracker,
        gyroManager: GyroManager,
        accelManager: AccelerometerManager,
        frameWidth: Int,
        frameHeight: Int
    ) {
        if (useRL) {
            // Build observation
            val observation = observationBuilder.build(
                trackingResult, tracker, busManager,
                gyroManager, accelManager, frameWidth, frameHeight
            )
            
            // Get RL action
            val action = rlController.predict(observation)
            
            // Execute action
            actionExecutor.execute(action)
        } else {
            // Fallback to PID
            pidController.update(trackingResult)
        }
    }
    
    fun toggleMode() {
        useRL = !useRL
        Log.d("Controller", "Mode: ${if (useRL) "RL" else "PID"}")
    }
}
```

---

# Ø§Ù„Ù…Ù„Ø§Ø­Ù‚

## Ù…Ù„Ø­Ù‚ Ø£: Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆØ§Ù„Ù…Ø®Ø±Ø¬Ø§Øª

### Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª (27 Ø¹Ù†ØµØ±)
| Ø§Ù„ÙÙ‡Ø±Ø³ | Ø§Ù„Ø­Ù‚Ù„ | Ø§Ù„Ù†Ø·Ø§Ù‚ | Ø§Ù„Ù…ØµØ¯Ø± |
|--------|-------|--------|--------|
| 0-1 | target_x, target_y | [-1, 1] | TrackingResult |
| 2-3 | target_w, target_h | [0, 1] | TrackingResult |
| 4-5 | target_vx, target_vy | [-1, 1] | ObjectTracker |
| 6 | confidence | [0, 1] | TrackingResult |
| 7 | target_visible | [0, 1] | StableTracker |
| 8 | frames_since_lost | [0, 1] | StableTracker |
| 9-11 | yaw, pitch, roll | [-1, 1] | SharedBusManager |
| 12-13 | yaw_vel, pitch_vel | [-1, 1] | computed |
| 14-16 | gyro_x, y, z | [-1, 1] | GyroManager |
| 17-19 | accel_x, y, z | [-1, 1] | AccelerometerManager |
| 20 | shake_level | [0, 1] | SensorFusionLayer |
| 21-22 | error_x, error_y | [-1, 1] | computed |
| 23-24 | integral_x, integral_y | [-1, 1] | computed |
| 25-26 | derivative_x, derivative_y | [-1, 1] | computed |

### Ø¬Ø¯ÙˆÙ„ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª (3 Ø¹Ù†Ø§ØµØ±)
| Ø§Ù„ÙÙ‡Ø±Ø³ | Ø§Ù„Ø­Ù‚Ù„ | Ø§Ù„Ù†Ø·Ø§Ù‚ | Ø§Ù„ÙˆØ¬Ù‡Ø© |
|--------|-------|--------|--------|
| 0 | yaw_delta | [-1, 1] | Servo 1 |
| 1 | pitch_delta | [-1, 1] | Servo 2 |
| 2 | roll_delta | [-1, 1] | Servo 3 |

---

## Ù…Ù„Ø­Ù‚ Ø¨: Ù…ØªØ·Ù„Ø¨Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨

| Ø§Ù„Ù…ØªØ·Ù„Ø¨ | Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ø¯Ù†Ù‰ | Ø§Ù„Ù…ÙˆØµÙ‰ |
|---------|-------------|--------|
| **GPU** | GTX 1060 | RTX 3080+ |
| **RAM** | 16 GB | 32 GB |
| **Python** | 3.8 | 3.10 |
| **PyTorch** | 1.13 | 2.0+ |
| **Stable-Baselines3** | 2.0 | 2.1+ |
| **ÙˆÙ‚Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨** | 2-4 Ø³Ø§Ø¹Ø§Øª | 6-12 Ø³Ø§Ø¹Ø© |
| **Ø¹Ø¯Ø¯ Ø§Ù„Ø®Ø·ÙˆØ§Øª** | 1M | 5M+ |

---

## Ù…Ù„Ø­Ù‚ Ø¬: Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø©

```bash
# Python packages
pip install gymnasium
pip install stable-baselines3[extra]
pip install tensorboard
pip install optuna
pip install numpy
pip install torch

# For Android deployment
pip install tensorflow
pip install tensorflow-lite
```

---

## Ù…Ù„Ø­Ù‚ Ø¯: Ù‡ÙŠÙƒÙ„ Ø§Ù„Ù…Ø¬Ù„Ø¯Ø§Øª

```
rl_project/
â”œâ”€â”€ envs/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ saqr_gimbal_env.py
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ checkpoints/
â”‚   â”œâ”€â”€ best/
â”‚   â””â”€â”€ rl_policy.tflite
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ tensorboard/
â”œâ”€â”€ scripts/
â”‚   â”œâ”€â”€ train_ppo.py
â”‚   â”œâ”€â”€ train_sac.py
â”‚   â””â”€â”€ export_tflite.py
â”œâ”€â”€ android/
â”‚   â””â”€â”€ app/src/main/java/.../
â”‚       â”œâ”€â”€ RLController.kt
â”‚       â”œâ”€â”€ ObservationBuilder.kt
â”‚       â””â”€â”€ ActionExecutor.kt
â””â”€â”€ README.md
```

---

# Ù†Ù‡Ø§ÙŠØ© Ø§Ù„Ø¯Ù„ÙŠÙ„

**Ø§Ù„Ø¥ØµØ¯Ø§Ø±**: 1.0  
**Ø¢Ø®Ø± ØªØ­Ø¯ÙŠØ«**: 2026-01-12  
**Ø§Ù„Ù…Ø¤Ù„Ù**: SAQR Seeker Development Team

---

> **Ù…Ù„Ø§Ø­Ø¸Ø©**: Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙŠÙˆÙØ± Ø§Ù„Ø£Ø³Ø§Ø³ Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆÙ†Ø´Ø± Ù†Ù…ÙˆØ°Ø¬ RL Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ù†Ø¸Ø§Ù… SAQR Seeker. Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø£ÙØ¶Ù„ Ø§Ù„Ù†ØªØ§Ø¦Ø¬ØŒ ÙŠÙÙ†ØµØ­ Ø¨Ø§Ù„Ø¨Ø¯Ø¡ Ø¨Ø§Ù„Ù…Ø­Ø§ÙƒØ§Ø© Ø«Ù… Ø§Ù„Ø§Ù†ØªÙ‚Ø§Ù„ ØªØ¯Ø±ÙŠØ¬ÙŠØ§Ù‹ Ù„Ù„Ù‡Ø§Ø±Ø¯ÙˆÙŠØ± Ø§Ù„Ø­Ù‚ÙŠÙ‚ÙŠ.
